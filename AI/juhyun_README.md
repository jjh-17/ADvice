## 광고 필터링 논문 요약

> 광고 글 필터링을 위해 데이터를 수집하고 `KorBERT`를 사용하여 문서분류를 학습하였다.
>
- 광고 필터링 기능 향상 방식 → **신고**

  광고 필터링 경로`Ad Filtering Path`를 통한 처리 과
  정에서 광고 분류의 성능의 한계로 오분류된 경우가 발
  생할 수 있다. 본 문제를 개선하기 위해 광고 필터링 개선
  경로(Ad Filtering Performance Improvement Path)
  를 고안했다. 해당 경로에서는 False Negative(FN),
  False Positive(FP) case와 같이 정상적으로 필터링 되
  지 않은 문서를 소비자가 신고하도록 유도하였다. 사용자
  는 오분류된 문서와 광고 유무를 체크하여 신고하게 되
  면 학습 데이터 양식의 라벨에 맞게 데이터가 가
  공되며 이는 데이터베이스에 누적된다.
  데이터베이스에 누적된 데이터 셋은 주기적으로 학습
  서버에서의 학습을 진행하여 개선된
  가중치를 광고 판단 시스템에 적용
  한다. 본 과정을 통해 광고 필터링 성능을 개선한다.

`Word2Vec` , `TF_IDF`  : 별로임

`GPT`, `ELMo` : 고려 대상

- `BERT`

  BERT 언어모델은 `어휘의 양방향 문맥 정보`와 `문장간 선후관계`를 학습하여 단어와 문맥을 반영한 벡터로 표현한다 이를 본 시스템에 적용하여 광고성 글에 대한 문맥을 파악하고 분류함.
  Google에서 공개한 다국어 `BERT`는 Fine-tuning만으로도 여러 응용이 가능하고 높은 성능을 보여주었으나 한국어에 특화되지 않았다. 2019년 ETRI에서 공개한 한국어 기반 언어모델인 `KorBERT`는 Google의 다국어 언어모델보다 한국어 분석 단위에 특화되었고 더 좋은 성능을 보였다. 이러한 성능 차이로 인해 본 시스템에는 한국어로 사전 학습된 KorBERT 언어모델을 사용한다.


## 광고 탐지 기술 선정
- 사람이 전혀 나오지 않는 작위적 이미지가 대다수인 경우 ⇒ 객체 탐지 모델(YOLOv8) or CNN
- 광고/협찬 임을 명시적으로 밝히는 경우(OCR)/ 학습한 모델에서 광고일 확률이 특정 값 이상인 경우/ 욕설이 나올 경우 ⇒ Kobert 텍스트 모델링

## KoBERT
* BERT : 구글에서 개발한 자연어 처리 모델로, 양방향으로 문맥을 이해하고 텍스트의 다양한 특징을 파악하여 자연어 이해 작업을 수행하는 데에 사용됨.
* KoBERT는 BERT 모델을 한국어에 특화되도록 조정한 모델
* Transformers 라이브러리를 사용하여 구현되어 있고 Transfer Learning을 기반으로 하며, 미리 학습된 언어 모델을 가져와서 특정한 자연어 처리 작업에 Fine-tuning하는 방식으로 사용됨 ⇒ 적은 양으로도 원하는 결과를 내기 쉬움

### 문제 상황
* 04/19 - 구글 colab에서 KoBERT를 가져와 사용하는 도중 의존성 문제, tokenizer import 시 에러 발생함
